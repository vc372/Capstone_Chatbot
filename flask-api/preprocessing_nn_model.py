# -*- coding: utf-8 -*-
"""Preprocessing_nn_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XILHTB7PXp7LzEbPyIEud_2PLRKFXumd
"""

#import statements
import numpy as np
import pandas as pd
import nltk
from nltk import tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import torch

#helper functions
def sentence_and_word_tokenize(doc):
    tokenizer = nltk.RegexpTokenizer(r'\w+|\S+')
    return [tokenizer.tokenize(sent) for sent in nltk.sent_tokenize(doc)]
     
def lemmatize(doc):
    lemmatizer = nltk.stem.WordNetLemmatizer()
    return [lemmatizer.lemmatize(word.lower().replace('\'', '')) for sent in doc for word in sent]

# def lemmatize_and_remove_stopwords(doc):
#     lemmatizer = nltk.stem.WordNetLemmatizer()
#     stopwords = nltk.corpus.stopwords.words('english')
#     return [lemmatizer.lemmatize(word.lower().replace('\'', '')) for sent in doc for word in sent if bool(word.lower() not in stopwords)]

def remove_stopwords(doc):
    stopwords = nltk.corpus.stopwords.words('english')
    return [word for word in doc if bool(word not in stopwords)]

def csr2tensor(csr_matrix):
    coo_matrix = csr_matrix.tocoo()
    values = coo_matrix.data
    indices = np.vstack((coo_matrix.row, coo_matrix.col))

    i = torch.LongTensor(indices)
    v = torch.FloatTensor(values)
    shape = coo_matrix.shape

    return torch.sparse.FloatTensor(i, v, torch.Size(shape)).to_dense()

def create_dataset(X,y):
    dataset = torch.utils.data.TensorDataset(X, y)
    trainloader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=20)
    return list(trainloader)

#preprocess function
def preprocess(message):
  d = {'text': [message]}
  test = pd.DataFrame(data=d)

  test['cleaned_text'] = test['text'].apply(lambda doc : re.sub(r'[\r\n]+', ' ', doc))
  test['cleaned_text'] = test['cleaned_text'].apply(lambda doc : re.sub(r'&nbsp', ' ', doc))
  test['cleaned_text'] = test['cleaned_text'].apply(lambda doc : re.sub(r"<.*?>", '', doc))
  test['cleaned_text'] = test['cleaned_text'].apply(lambda doc : re.sub(r"[â€,;@#?!$/\"-:\(\)><]+", " ", doc))
  tokenized_text = test['cleaned_text'].apply(lambda doc : sentence_and_word_tokenize(doc))
  test['cleaned_text'] = tokenized_text.apply(lambda doc : lemmatize(doc))
  test['cleaned_text_no_stopwords'] = test['cleaned_text'].apply(lambda doc : remove_stopwords(doc))
  #tagged_text = tokenized_text.apply(lambda doc : [nltk.pos_tag(sent) for sent in doc])
  X_no_stopwords = test['cleaned_text_no_stopwords']

  return X_no_stopwords
